{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVmYbqqUSlCH"
      },
      "source": [
        "# Fine-tuning Florence-2 on VizWiz VQA\n",
        "\n",
        "In this notebook, we will fine-tune Florence-2 by MSFT, a new vision language model capable of various tasks, on vizwiz question answering dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GtBnwuQ65oC",
        "outputId": "54d344a9-468f-4a00-d3df-8dcd05f1ceda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nHXXTq-7M3b",
        "outputId": "05ab4ed5-f9b5-4018-aa61-60211ec56936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/32933-research-project\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/32933-research-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y6b1dvjgYXOD"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets flash_attn timm einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KPNGpiyS78K-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import argparse\n",
        "import json\n",
        "from collections import Counter\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp13B-8Myjf"
      },
      "source": [
        "We can load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor`Â  classes of transformers library. Note that we need to pass `trust_remote_code`Â as `True`Â since this model is not a transformers model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqDWEWDcaSxN",
        "outputId": "058f4ce7-df85-47d9-edfb-64d9f667c1b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kvSWOIHsab7o"
      },
      "outputs": [],
      "source": [
        "# Function to run the model on an example\n",
        "def run_example(task_prompt, text_input, image):\n",
        "    prompt = task_prompt + text_input\n",
        "\n",
        "    # Ensure the image is in RGB mode\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
        "    return parsed_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ojQNLbhd_BDl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VizWizDataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_file):\n",
        "        with open(annotation_file, \"r\") as f:\n",
        "            self.samples = json.load(f)\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        # Prefix the question similar to DocVQADataset (change prefix as needed)\n",
        "        question = \"<VizWiz>\" + sample[\"question\"].strip()\n",
        "\n",
        "        # Extract the first answer; if answers is empty or missing, use a fallback.\n",
        "        if sample.get(\"answers\") and len(sample[\"answers\"]) > 0:\n",
        "            # Here, we assume each answer is a dictionary with the key \"answer\".\n",
        "            first_answer = sample[\"answers\"][0].get(\"answer\", \"unanswerable\")\n",
        "        else:\n",
        "            first_answer = \"unanswerable\"\n",
        "\n",
        "        # Build the image path and open the image.\n",
        "        image_path = os.path.join(self.image_dir, sample[\"image\"])\n",
        "        image = Image.open(image_path)\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        return question, first_answer, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ilMb0ivGdt9l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "# Assume processor and device are defined earlier, for example:\n",
        "# processor = AutoProcessor.from_pretrained(\"your-model-id\")\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(\n",
        "        text=list(questions),\n",
        "        images=list(images),\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "    return inputs, answers\n",
        "\n",
        "# Paths specific to VizWiz\n",
        "vizwiz_train_image_dir = \"/content/drive/MyDrive/32933-research-project/vqa-vizwiz/data/balanced_subset2/train\"  # Folder with your VizWiz images\n",
        "vizwiz_val_image_dir = \"/content/drive/MyDrive/32933-research-project/vqa-vizwiz/data/balanced_subset2/val\"  # Folder with your VizWiz images\n",
        "vizwiz_train_annotation_file = \"/content/drive/MyDrive/32933-research-project/vqa-vizwiz/data/balanced_subset2/annotations/train.json\"\n",
        "vizwiz_val_annotation_file = \"/content/drive/MyDrive/32933-research-project/vqa-vizwiz/data/balanced_subset2/annotations/val.json\"\n",
        "\n",
        "# Create datasets using the VizWizDataset class\n",
        "train_dataset = VizWizDataset(vizwiz_train_image_dir, vizwiz_train_annotation_file)\n",
        "val_dataset = VizWizDataset(vizwiz_val_image_dir, vizwiz_val_annotation_file)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 1\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv_BBNnJAhyd",
        "outputId": "d1c7ed4b-7d3a-47be-9ab9-4ec40dc828c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.51.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bC06Mc7jOdpY"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        i = -1\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "            i += 1\n",
        "            inputs, answers = batch\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "                inputs, answers = batch\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TSScAwP4eryt"
      },
      "outputs": [],
      "source": [
        "for param in model.vision_tower.parameters():\n",
        "  param.is_trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZybGHd3fNJ1",
        "outputId": "4851fe4a-40dd-428d-d5c4-cc9f0c1496eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2448/2448 [28:49<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.6106407959090742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [02:05<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 0.5259570672915704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2448/2448 [28:49<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.42579746650686157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [02:06<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 0.5263055362323217\n"
          ]
        }
      ],
      "source": [
        "train_model(train_loader, val_loader, model, processor, epochs=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
