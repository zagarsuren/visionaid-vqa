# VisionAid-VQA: Inclusive Visual Question Answering Using Deep Learning and Multimodal Attention Mechanisms

This Streamlit web application allows users to upload or capture an image, ask a question about it, and receive **text and audio responses** using advanced **Visual Question Answering (VQA)** models. The system is designed for accessibility, especially supporting visually impaired users.

![img](assets/demo/home.jpeg)

---

## ğŸ§  Features

- ğŸ” Supports VQA models:
  - `vilt_finetuned_vizwiz` (Vision-and-Language Transformer model finetuned with VizWiz)
  - `florence2-finetuned` (Multimodal reasoning model finetuned with VizWiz)
- ğŸ“· Accepts image input from upload or camera
- â“ Accepts natural language questions 
- ğŸ”Š Converts answers to speech using `gTTS`
- ğŸ§ Auto-plays audio response in the app

---

## ğŸ“ Project Structure

```graphql
project_root/
â”œâ”€â”€ data  
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ robust_vilt.py
â”‚   â”œâ”€â”€ florence2.py
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ vilt_finetuned_vizwiz
â”‚   â”œâ”€â”€ florence2-finetuned
â”œâ”€â”€ scripts
â”œâ”€â”€ app.py
```

## Setup
---
1. Clone the repository:
```bash
git clone https://github.com/your-username/inclusive-vqa-app.git
cd visionaid-vqa
```

2. Create a Virtual Environment
Upgrade pip:
```bash
pip install --upgrade pip
```

Create and activate Virtual Environment (Windows):
```bash
python -m venv vqa
vqa\Scripts\activate
```

Create and activate Virtual Environment (Linux):
```bash
python3 -m venv vqa
source vqa/bin/activate
```

3. Install the dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ§  Model Weights

- `ViLT` â†’ `/models/vilt_finetuned_vizwiz`
- `Florence2Model` â†’ `/models/florence2-finetuned`

## To run the Web App
```bash
streamlit run app.py
```

## License
This project is licensed under the MIT License.